{
  "metadata": {
    "name": "Jarvis Hive Project",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "\u003c!--- QUESTION 1: Load GS Data to HDFS --\u003e\n# QUESTION 1: Load GS Data to HDFS\n\n## Step 1: Checking HDFS home directory\n### Show hdfs home directories\n`hdfs dfs -ls /user`\n### Create a directory for working with HDFS if not exist\n`hdfs dfs -mkdir /user/phuong`\n    \n## Create an empty external table in Hive\n\n    DROP TABLE IF EXISTS wdi_csv_text;\n    CREATE EXTERNAL TABLE wdi_csv_text\n    (year INTEGER, countryName STRING, countryCode STRING, indicatorName STRING, indicatorCode STRING, indicatorValue FLOAT)\n    ROW FORMAT DELIMITED FIELDS TERMINATED BY \u0027,\u0027 LINES TERMINATED BY \u0027\\n\u0027\n    LOCATION \u0027hdfs:///user/phuong/hive/wdi/wdi_csv_text\u0027;\nNote:\n- Row FORMAT ... : declare `,` as field delimiter and `\\n` as line terminator\n- LOCATION: the resource location\n- Test if the table is created: `show tables;` and `describe formatted wdi_csv_text;`\n\n## Step 2: Overwrite the empty table by the wdi_gs table\n    INSERT OVERWRITE TABLE wdi_csv_text\n    SELECT * FROM wdi_gs;\n\n## Step 3: Check if files are copied to the HDFS directory\n### Show files and their size\n    hdfs dfs -ls -h /user/phuong/hive/wdi/wdi_csv_text;\n### Show total disk usage of the directory\n    hdfs dfs -du -s -h /user/phuong/hive/wdi/wdi_csv_text;\n\n## Step 4: Execute a querry to check wdi_csv_text content\n\n    SELECT count(countryName) FROM wdi_csv_text;\nNote: Execute it at least 2 times to see the difference in performance\n   \n## Step 5: Clean filesystem cache\n    echo 1 | sudo tee /proc/sys/vm/drop_caches\nNote: tee cmd helps to write the input (echo 1) to both the standard output (terminal) and the file drop_caches (interface for clearing filesystem cache)\n\n## Step 6: Execute the querry in step 4 again to see the difference in performance\nAt this point, you should see its performance is like the first time."
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%hive\nDROP TABLE IF EXISTS wdi_csv_text;\nCREATE EXTERNAL TABLE wdi_csv_text\n(year INTEGER, countryName STRING, countryCode STRING, indicatorName STRING, indicatorCode STRING, indicatorValue FLOAT)\nROW FORMAT DELIMITED FIELDS TERMINATED BY \u0027,\u0027 LINES TERMINATED BY \u0027\\n\u0027\nLOCATION \u0027hdfs:///user/phuong/hive/wdi/wdi_csv_text\u0027;\n"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%hive\nINSERT OVERWRITE TABLE wdi_csv_text\nSELECT * FROM wdi_gs;"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "\u003c!--- QUESTION 2: Monitor Hadoop/Yarn job --\u003e\n# QUESTION 2: Monitor Hadoop/Yarn job\n\n## Monitor Hadoop/Yarn job \n### Using Yarn Application Timeline in the Web Interfaces tab\n\n###  Tez in the Web Interfaces tab\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# QUESTION 3: Hive vs Bash\n## Perform row count using Bash\n    cd ~\n    hdfs  dfs -get  hdfs:///user/phuong/hive/wdi/wdi_csv_text .\n    cd wdi_csv_text\n    du -ch .\n    \n    echo 3 | sudo tee /proc/sys/vm/drop_caches\n    date +%s \u0026\u0026 cat * | wc \u0026\u0026 date +%s\n    \n-  move to home directory\n-  copy wdi_csv_text dir to current dir\n-  calculate current directory size. `-c`: grand total `-h`: humand readable format\n-  clear filessystem cache\n-   counting row. `date+%s`: current datetime in epoch seconds, `wc`: word count (return newline - word - byte counts)\n\n## Perform row count using Hive\n    SELECT count(countryName) FROM wdi_csv_text;\n## Conclusion\n- Bash is not using filesystem cache. Therefore the performance remains the same with or without filesystem cache\n- Hive uses filesystem cache which allow to read the recently read files from memory instead of disk. Hence, improve the performance."
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sh\ncd ~\nhdfs  dfs -get  hdfs:///user/phuong/hive/wdi/wdi_csv_text .\ncd wdi_csv_text\ndu -ch .\n\necho 3 | sudo tee /proc/sys/vm/drop_caches\ndate +%s \u0026\u0026 cat * | wc \u0026\u0026 date +%s\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# QUESTION 4: Parsing Issue\n\n## Step 1: Execute below statement\n    SELECT distinct(indicatorcode)\n    FROM wdi_csv_text\n    ORDER BY indicatorcode\n    LIMIT 20;\nNote: the returned values are not indicatorCode\n## Step 2: Execute another statement to check\n    SELECT indicatorcode\n    FROM wdi_csv_text\n    ORDER BY indicator\n    LIMIT 200;\n## Step 3: Compare conclusion\nThere are wrong values in the indicatorcode column. `select distinct` will return only distinct values\n## Step 4: Make a new external table with one column only\n    DROP TABLE IF EXISTS wdi_gs_debug;\n    CREATE EXTERNAL TABLE wdi_gs_debug \n    (line String)\n    ROW FORMAT DELIMITED LINES TERMINATED BY \u0027\\n\u0027\n    LOCATION \u0027gs://jarvis_dataeng_minh_phuong/datasets/wdi_2016\u0027;\nNote: No fields delimiter decalred and the data source is from Google Storage\n## Step 5: Check the content of debug table\n    SELECT * FROM wdi_gs_debug \n    WHERE line like \"%\\(\\% of urban population\\)\\\"%\" LIMIT 20;\nNote:Now you can see the problem is that in the indicatorName field, there are `,` which will be misunderstood as field delimiters\n\n## Step 6: Create external table `wdi_opencsv_gs` using OpenCsvSerDe from GS files\n    DROP TABLE IF EXISTS wdi_opencsv_gs;\n    CREATE EXTERNAL TABLE wdi_opencsv_gs\n    (year INTEGER, countryName STRING, countryCode STRING, indicatorName STRING, indicatorCode STRING, indicatorValue FLOAT)\n    ROW FORMAT SERDE \u0027org.apache.hadoop.hive.serde2.OpenCSVSerde\u0027\n    STORED AS TEXTFILE\n    LOCATION \u0027gs://jarvis_dataeng_minh_phuong/datasets/wdi_2016\u0027;\n### Default delimiter of OpenCsvSerDe are\n    DEFAULT_ESCAPE_CHARACTER \\\n    DEFAULT_QUOTE_CHARACTER  \"\n    DEFAULT_SEPARATOR        ,\n\nTo change the default values, place this right below the `Row FORMAT SERDE` line:  \n\n    WITH SERDEPROPERTIES (\n       \"separatorChar\" \u003d \"\\t\",\n       \"quoteChar\"     \u003d \"\u0027\",\n       \"escapeChar\"    \u003d \"\\\\\"\n    )  \n## Step 7: Create empty external table `wdi_opencsv_text` for later overwrite data from `wdi_opencsv_gs`\n    DROP TABLE IF EXISTS wdi_opencsv_text;\n    CREATE EXTERNAL TABLE wdi_opencsv_text\n    (year INTEGER, countryName STRING, countryCode STRING, indicatorName STRING, indicatorCode STRING, indicatorValue FLOAT)\n    ROW FORMAT SERDE \u0027org.apache.hadoop.hive.serde2.OpenCSVSerde\u0027\n    STORED AS TEXTFILE\n    LOCATION \u0027hdfs:///user/phuong/hive/wdi/wdi_opencsv_text\u0027;\n    \n## Step 8: Overwrite data from `wdi_opencsv_gs` to `wdi_opencsv_text`\n    INSERT OVERWRITE TABLE wdi_opencsv_text\n    SELECT * FROM wdi_opencsv_gs;\n\n## Step 9: Check the indicatorCode field again\n    SELECT distinct(indicatorcode) FROM wdi_opencsv_text LIMIT 20;\nThere should be no more parsing issue now.\n\n## Step 10: Compare execution time of `wdi_opencsv_gs` and `wdi_opencsv_text`\n    SELECT distinct(indicatorcode) FROM wdi_opencsv_text;\n    SELECT distinct(indicatorcode) FROM wdi_csv_text;\nNote: \n- The `wdi_csv_text` should be much faster than `wdi_opencsv_text ` since it uses the default `LazySimpleSerDe` SerDe Library. `OpenCSVSerde` gives a more comprehensive Serializer-Deserializer. To see the SerDe Library, use `Describe formatted table_name`\n- `OpenCSVSerde` will consider all fields are `String` regardless of the column data type. To cast the column to a desired data type, make a `view` from the table and cast the columns to the dedired type.\n"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%hive\nSELECT distinct(indicatorcode)\nFROM wdi_csv_text\nORDER BY indicatorcode\nLIMIT 20;"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%hive\nDROP TABLE IF EXISTS wdi_gs_debug;\nCREATE EXTERNAL TABLE wdi_gs_debug \n(line String)\nROW FORMAT DELIMITED LINES TERMINATED BY \u0027\\n\u0027\nLOCATION \u0027gs://jarvis_dataeng_minh_phuong/datasets/wdi_2016\u0027;\n"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%hive\nDROP TABLE IF EXISTS wdi_opencsv_gs;\nCREATE EXTERNAL TABLE wdi_opencsv_gs\n(year INTEGER, countryName STRING, countryCode STRING, indicatorName STRING, indicatorCode STRING, indicatorValue FLOAT)\nROW FORMAT SERDE \u0027org.apache.hadoop.hive.serde2.OpenCSVSerde\u0027\nSTORED AS TEXTFILE\nLOCATION \u0027gs://jarvis_dataeng_minh_phuong/datasets/wdi_2016\u0027;"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%hive\nDROP TABLE IF EXISTS wdi_opencsv_text;\nCREATE EXTERNAL TABLE wdi_opencsv_text\n(year INTEGER, countryName STRING, countryCode STRING, indicatorName STRING, indicatorCode STRING, indicatorValue FLOAT)\nROW FORMAT SERDE \u0027org.apache.hadoop.hive.serde2.OpenCSVSerde\u0027\nSTORED AS TEXTFILE\nLOCATION \u0027hdfs:///user/phuong/hive/wdi/wdi_opencsv_text\u0027;"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%hive\nINSERT OVERWRITE TABLE wdi_opencsv_text\nSELECT * FROM wdi_opencsv_gs;"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%hive\nSELECT distinct(indicatorcode) FROM wdi_opencsv_text LIMIT 20;"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# QUESTION 5: OpenCSVSerde limitaion\n## Limitation\n`OpenCSVSerde` will consider all fields are `String` regardless of the column data type. To cast the column to a desired data type, make a `view` from the table and cast the columns to the dedired type.\n\n## Show the diff between the two tables \n    DESCRIBE FORMATTED wdi_opencsv_text;\n    DESCRIBE FORMATTED wdi_csv_text;\nNote: check data type of the year and indicatorValue fields in the two tables\n## Create a view from `wdi_opencsv_text` table for casting columns data type\n    DROP VIEW IF EXISTS wdi_opencsv_text_view;\n    CREATE VIEW wdi_opencsv_text_view AS \n    SELECT CAST(year AS INTEGER) , countryName, countryCode, indicatorName, indicatorCode, CAST(indicatorValue AS FLOAT)\n    FROM wdi_opencsv_text;\n## Check the view fields data types\n    DESCRIBE FORMATTED wdi_opencsv_text_view;"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%hive\nDESCRIBE FORMATTED wdi_opencsv_text;\nDESCRIBE FORMATTED wdi_csv_text;"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%hive\nDROP VIEW IF EXISTS wdi_opencsv_text_view;\nCREATE VIEW wdi_opencsv_text_view AS \nSELECT CAST(year AS INTEGER) , countryName, countryCode, indicatorName, indicatorCode, CAST(indicatorValue AS FLOAT)\nFROM wdi_opencsv_text;"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%hive\nDESCRIBE FORMATTED wdi_opencsv_text_view;"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# QUESTION 6: 2015 Canada GDP Growth HQL\n## Search for Canada indicator Name/Code\n    SELECT countryname, indicatorname, indicatorCode \n    FROM wdi_opencsv_text \n    WHERE UPPER(countryName)\u003d \"CANADA\" and UPPER(indicatorname) like \"%GDP GROWTH%\"\n    GROUP BY countryname, indicatorname, indicatorcode;\n## Search for Canada GDP Growth 2015\n    SELECT indicatorvalue, countryname, year \n    FROM wdi_opencsv_text\n    WHERE UPPER(countryname)\u003d\"CANADA\" AND indicatorcode\u003d\"NY.GDP.MKTP.KD.ZG\" AND year\u003d\"2015\";\n## Discuss to improve the querries performance\n- It reads data from external source over OpenCSVSerde makes it takes long to process querries\n- Indexing/partition the table can improve querry performance"
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%hive\nSELECT countryname, indicatorname, indicatorCode \nFROM wdi_opencsv_text \nWHERE UPPER(countryName)\u003d \"CANADA\" and UPPER(indicatorname) like \"%GDP GROWTH%\"\nGROUP BY countryname, indicatorname, indicatorcode;"
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%hive\nSELECT indicatorvalue, countryname, year \nFROM wdi_opencsv_text\nWHERE UPPER(countryname)\u003d\"CANADA\" AND indicatorcode\u003d\"NY.GDP.MKTP.KD.ZG\" AND year\u003d\"2015\";"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# QUESTION 7: Hive Partitions\n## Create `wdi_opencsv_text_partitions` table partitioned by `year`\n    DROP TABLE IF EXISTS wdi_opencsv_text_partitions;\n    \n    CREATE EXTERNAL TABLE wdi_opencsv_text_partitions\n    (countryName STRING, countryCode STRING, indicatorName STRING, indicatorCode STRING, indicatorValue String)\n    PARTITIONED BY (year String)\n    ROW FORMAT SERDE \u0027org.apache.hadoop.hive.serde2.OpenCSVSerde\u0027\n    STORED AS TEXTFILE\n    LOCATION \u0027hdfs:///user/phuong/hive/wdi/wdi_opencsv_text_partitions\u0027;\n## Load data from `wdi_opencsv_text` to `wdi_opencsv_text_partitions` using dynamic partition\n    SET hive.exec.dynamic.partition.mode\u003dnonstrict;\n    SET hive.stats.column.autogather\u003dfalse;\n    FROM wdi_opencsv_text\n    INSERT OVERWRITE TABLE wdi_opencsv_text_partitions\n    PARTITION(year)\n    SELECT countryName, countryCode, indicatorName, indicatorCode, indicatorValue, year;\nNotes:\n- `Set hive.exec.max.dynamic.partitions.pernode\u003d1000` to set max amount of partition can be created per mapper/reducer. It\u0027s 100 by default.\n- `hive.exec.max.dynamic.partitions` : total amount of partitions can be created per querry\n- `hive.exec.max.created.files`: total amount of files can be created by all mapper/reducer\n- `SET hive.stats.column.autogather\u003dfalse` to turn of autogather column stats\n- Make sure the year (parition) column is at the end\n\n## Inspect amount of partitions have been created in `wdi_opencsv_text_partitions` table\n    hdfs dfs -count -h /user/phuong/hive/wdi/wdi_opencsv_text_partitions;\nNote: above cmd return: count of directories, files, total_size and the container_name\n## Re-run the Canada GDP Growth 2015 against the `wdi_opencsv_text_partitions` table\n    SELECT indicatorvalue, countryname, year \n    FROM wdi_opencsv_text_partitions\n    WHERE UPPER(countryname)\u003d\"CANADA\" AND indicatorcode\u003d\"NY.GDP.MKTP.KD.ZG\" AND year\u003d\"2015\";\nNote: the performance must be a lot better compared to querry with the non-partitioned table"
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%hive\nDROP TABLE IF EXISTS wdi_opencsv_text_partitions;\nCREATE EXTERNAL TABLE wdi_opencsv_text_partitions\n(countryName STRING, countryCode STRING, indicatorName STRING, indicatorCode STRING, indicatorValue FLOAT)\nPARTITIONED BY (year INTEGER)\nROW FORMAT SERDE \u0027org.apache.hadoop.hive.serde2.OpenCSVSerde\u0027\nLOCATION \u0027hdfs:///user/phuong/hive/wdi/wdi_opencsv_text_partitions\u0027;"
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%hive\nSET hive.exec.dynamic.partition.mode\u003dnonstrict;\nset hive.stats.column.autogather\u003dfalse;\nFROM wdi_opencsv_text\nINSERT OVERWRITE TABLE wdi_opencsv_text_partitions\nPARTITION(year)\nSELECT countryName, countryCode, indicatorName, indicatorCode, indicatorValue, year;"
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sh\nhdfs dfs -count -h /user/phuong/hive/wdi/wdi_opencsv_text_partitions;"
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%hive\nSELECT indicatorvalue, countryname, year \nFROM wdi_opencsv_text_partitions\nWHERE UPPER(countryname)\u003d\"CANADA\" AND indicatorcode\u003d\"NY.GDP.MKTP.KD.ZG\" AND year\u003d\"2015\";"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# QUESTION 8: Columnar File Optimization (optimize HQL using Columnar file)\n## Create `wdi_csv_parquet` table with Parquet file\n    DROP TABLE IF EXISTS wdi_csv_parquet;\n    CREATE EXTERNAL TABLE wdi_csv_parquet\n    (year INTEGER, countryName STRING, countryCode STRING, indicatorName STRING, indicatorCode STRING, indicatorValue FLOAT)\n    STORED AS PARQUET\n    LOCATION \u0027hdfs:///user/phuong/hive/wdi/wdi_csv_parquet\u0027;     \n## Load data from `wdi_opencsv_gs` to the partquet table\n    FROM wdi_opencsv_gs\n    INSERT OVERWRITE TABLE wdi_csv_parquet\n    SELECT *;\n## Compare file size between `wdi_csv_parquet` and `wdi_opencsv_text`\n    hdfs dfs -du -s -h /user/phuong/hive/wdi/wdi_opencsv_text;\n    hdfs dfs -du -s -h /user/phuong/hive/wdi/wdi_csv_parquet;\nNote: parquet files should be a lot smaller than text files\n## Compare runtime\n    SELECT count(countryName) FROM wdi_csv_parquet;\n    SELECT count(countryName) FROM wdi_opencsv_text;\nNotes:\n- The first execution on both tables are not much different (parquet gave a slight faster)\n- The second execution onward, parquet table give almost 4 times faster than the text table (26 seconds vs 99 seconds)\n## Execute `2015 GDP Growth` HQL against `wdi_csv_parquet` and `wdi_opencsv_text` tables, and then compare performance.\n    SELECT indicatorvalue, countryname, year \n    FROM wdi_csv_parquet\n    WHERE UPPER(countryname)\u003d\"CANADA\" AND indicatorcode\u003d\"NY.GDP.MKTP.KD.ZG\" AND year\u003d\"2015\";\nNotes:\n- Same as the above comparison, it gave much faster performance sicne the second execution."
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%hive\n DROP TABLE IF EXISTS wdi_csv_parquet;\nCREATE EXTERNAL TABLE wdi_csv_parquet\n(year INTEGER, countryName STRING, countryCode STRING, indicatorName STRING, indicatorCode STRING, indicatorValue FLOAT)\nSTORED AS PARQUET\nLOCATION \u0027hdfs:///user/phuong/hive/wdi/wdi_csv_parquet\u0027;  "
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%hive\nFROM wdi_opencsv_gs\nINSERT OVERWRITE TABLE wdi_csv_parquet\nSELECT *;"
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sh\nhdfs dfs -du -s -h /user/phuong/hive/wdi/wdi_opencsv_text;\nhdfs dfs -du -s -h /user/phuong/hive/wdi/wdi_csv_parquet;"
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%hive\nSELECT count(countryName) FROM wdi_csv_parquet;"
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%hive\nSELECT count(countryName) FROM wdi_opencsv_text;"
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%hive\nSELECT indicatorvalue, countryname, year \nFROM wdi_csv_parquet\nWHERE UPPER(countryname)\u003d\"CANADA\" AND indicatorcode\u003d\"NY.GDP.MKTP.KD.ZG\" AND year\u003d\"2015\";"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# QUESTION 9: Find Highest GDP Growth using Hive SQL and Spark SQL\n## Use Hive SQL\n    SELECT t1.indicatorValue, t1.year, t1.countryName \n    FROM wdi_csv_parquet t1\n    INNER JOIN (\n        SELECT MAX(indicatorValue) AS maxValue, countryName\n        FROM wdi_csv_parquet\n        WHERE indicatorCode \u003d \"NY.GDP.MKTP.KD.ZG\" AND indicatorValue \u003c\u003e 0\n        GROUP BY countryName\n    ) t2\n    ON t1.indicatorValue \u003d t2.maxValue AND t1.countryName \u003d t2.countryName;\nNotes:\n- Find the max GDP Growth of each country\n- Join the result set with the current table on the maxValues and countryName for picking out the year.\n\n## Use Spark SQL\n    SELECT t1.indicatorValue, t1.year, t1.countryName \n    FROM wdi_csv_parquet t1\n    INNER JOIN (\n        SELECT MAX(indicatorValue) AS maxValue, countryName\n        FROM wdi_csv_parquet\n        WHERE indicatorCode \u003d \"NY.GDP.MKTP.KD.ZG\" AND indicatorValue \u003c\u003e 0\n        GROUP BY countryName\n    ) t2\n    ON t1.indicatorValue \u003d t2.maxValue AND t1.countryName \u003d t2.countryName;\n"
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%hive\nSELECT t1.indicatorValue, t1.year, t1.countryName \nFROM wdi_csv_parquet t1\nINNER JOIN (\n    SELECT MAX(indicatorValue) AS maxValue, countryName\n    FROM wdi_csv_parquet\n    WHERE indicatorCode \u003d \"NY.GDP.MKTP.KD.ZG\" AND indicatorValue \u003c\u003e 0\n    GROUP BY countryName\n) t2\nON t1.indicatorValue \u003d t2.maxValue AND t1.countryName \u003d t2.countryName;\n"
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.sql\nSELECT t1.indicatorValue, t1.year, t1.countryName \nFROM wdi_csv_parquet t1\nINNER JOIN (\n    SELECT MAX(indicatorValue) AS maxValue, countryName\n    FROM wdi_csv_parquet\n    WHERE indicatorCode \u003d \"NY.GDP.MKTP.KD.ZG\" AND indicatorValue \u003c\u003e 0\n    GROUP BY countryName\n) t2\nON t1.indicatorValue \u003d t2.maxValue AND t1.countryName \u003d t2.countryName;"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# QUESTION 10: Sort GDP by Country and Year\n## Sort by countryName and Year, return GDP Growth, CountryName, Year and indicatorCode\n    SELECT countryName, Year, indicatorCode, indicatorValue\n    FROM wdi_csv_parquet\n    WHERE indicatorCode \u003d \"NY.GDP.MKTP.KD.ZG\"\n    ORDER BY countryName, year;\n"
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%hive\nSELECT countryName, Year, indicatorCode, indicatorValue\nFROM wdi_csv_parquet\nWHERE indicatorCode \u003d \"NY.GDP.MKTP.KD.ZG\"\nORDER BY countryName, year;"
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%hive\nSELECT countryName, Year, indicatorCode, indicatorValue\nFROM wdi_csv_parquet\nWHERE indicatorCode \u003d \"NY.GDP.MKTP.KD.ZG\"\nSORT BY countryName, year;"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%hive\n"
    }
  ]
}