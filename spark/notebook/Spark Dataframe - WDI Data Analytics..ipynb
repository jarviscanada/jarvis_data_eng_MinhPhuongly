{
  "metadata": {
    "name": "Spark Dataframe - WDI Data Analytics",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# SparkSession\n\nSparkSession is automatically created when you start up a Notebook (e.g. Zeppelin, Databricks)\n\n\u003cimg src\u003d\"https://i.imgur.com/5Ai45fb.jpg\" width\u003d500px\u003e"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\n//Scala SparkSession\nspark"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n#PySpark SparkSession\nspark"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": " \n\n# Show DataFrame\n\n`df.show()` is the Spark native API that displays data but it\u0027s not pretty. \n\n`z.show(df)` is a Zeppelin build-in feature that allows you to show a df result in a pretty way"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n\n#List all hive tables in a df\ntables_df \u003d spark.sql(\"show tables\")"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n\ntables_df.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\nz.show(tables_df)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": " \n\n# Spark SQL vs Dataframe\n\n`%sql` is the Spark SQL interpreter\n\n`%spark.pyspark` is the PySpark interpreter\n\n`%spark` is the Spark Scala interpreter"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sql\n\nselect count(1) from wdi_csv_parquet"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n\n#Read Hive data to a df (this is lazy)\nwdi_df \u003d spark.sql(\"SELECT * from wdi_csv_parquet\")\n#Persist df in memory for fast futuer access\nwdi_df \u003d wdi_df.cache()\nwdi_df.printSchema()\n\n#Spark action is eager\nz.show(wdi_df.count())"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": " \n\n# Show Historical GDP for Canada\n\n- Re-write the hive query (left cell) using PySpark df"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sql\nSELECT year, IndicatorValue as GDP\nFROM wdi_csv_parquet\nWHERE indicatorCode \u003d \u0027NY.GDP.MKTP.KD.ZG\u0027 and countryName \u003d \u0027Canada\u0027\nORDER BY year\n"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n#Find GDP growth of Canada from wdi_csv_parquet table\ndf \u003d spark.sql(\"SELECT * FROM wdi_csv_parquet\")\n\n#persist data\ndf \u003d df.cache()\n#show data from cache\ndf.printSchema()\n#use z.show to display df result and draw a bar chart\nwdi_canada_df \u003d df.selectExpr(\"year\", \"IndicatorValue as GDP\").filter( (df[\"indicatorCode\"] \u003d\u003d \"NY.GDP.MKTP.KD.ZG\") \u0026 (df[\"countryName\"] \u003d\u003d \"Canada\") ).orderBy(\"year\")\nz.show(wdi_canada_df)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": " \n\n# Show GDP for Each County and Sort By Year\n\n- Re-write the hive query (left cell) using PySpark df  \n    - hint: you can create multiple DFs "
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sql\nSELECT countryname,\n       year,\n       indicatorcode,\n       indicatorvalue\nFROM wdi_csv_parquet\nWHERE indicatorcode \u003d \u0027NY.GDP.MKTP.KD.ZG\u0027\nDISTRIBUTE BY countryname\nSORT BY countryname, year\n"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n\n#GPD of each country over each year in the table\ndf \u003d spark.sql(\"SELECT countryname, \\\n           year, \\\n           indicatorcode, \\\n           indicatorvalue \\\n    FROM wdi_csv_parquet\")\n#Check amount of Partition of the dataframe\nz.show(df.rdd.getNumPartitions())\n\n#Use PySpark APIs to get the desire dataframe\nwdi_world_df \u003d df.select(\"*\").filter( df[\"indicatorcode\"] \u003d\u003d \"NY.GDP.MKTP.KD.ZG\").sort(\"countryName\",\"year\")\n\nz.show(wdi_world_df)\n\n\n\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Find the highest GDP for each country\n\n- Re-write the hive query (left cell) using PySpark df\n"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sql\n\nSELECT wdi_csv_parquet.indicatorvalue AS value, \n       wdi_csv_parquet.year           AS year, \n       wdi_csv_parquet.countryname    AS country \nFROM   (SELECT Max(indicatorvalue) AS ind, \n               countryname \n        FROM   wdi_csv_parquet \n        WHERE  indicatorcode \u003d \u0027NY.GDP.MKTP.KD.ZG\u0027 \n               AND indicatorvalue \u003c\u003e 0 \n        GROUP  BY countryname) t1 \n       INNER JOIN wdi_csv_parquet \n               ON t1.ind \u003d wdi_csv_parquet.indicatorvalue \n                  AND t1.countryname \u003d wdi_csv_parquet.countryname\n"
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n\n#Max GDP of each country over the years\n\n\ndf1 \u003d spark.sql(\"SELECT Max(indicatorvalue) AS ind, countryname AS cName \\\n        FROM   wdi_csv_parquet \\\n        WHERE  indicatorcode \u003d \u0027NY.GDP.MKTP.KD.ZG\u0027 \\\n               AND indicatorvalue \u003c\u003e 0 \\\n        GROUP  BY countryname\")\n\ndf1.printSchema()\n\ntemp_df \u003d spark.sql(\"SELECT * FROM wdi_csv_parquet\")\ntemp_df.printSchema()\n\ndf2 \u003d temp_df.join(df1).filter( (temp_df[\"indicatorValue\"]\u003d\u003ddf1[\"ind\"]) \u0026 (temp_df[\"countryName\"]\u003d\u003ddf1[\"cName\"])).select(\"indicatorvalue\",\"year\",\"countryName\")\n\nz.show(df2)\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.pyspark\n"
    }
  ]
}